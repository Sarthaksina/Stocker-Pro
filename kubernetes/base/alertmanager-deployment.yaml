apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager
  labels:
    app: alertmanager
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
    spec:
      containers:
      - name: alertmanager
        image: prom/alertmanager:v0.25.0
        imagePullPolicy: IfNotPresent
        args:
        - "--config.file=/etc/alertmanager/alertmanager.yml"
        - "--storage.path=/alertmanager"
        ports:
        - containerPort: 9093
          name: http
        readinessProbe:
          httpGet:
            path: /-/ready
            port: http
          initialDelaySeconds: 30
          timeoutSeconds: 30
        livenessProbe:
          httpGet:
            path: /-/healthy
            port: http
          initialDelaySeconds: 30
          timeoutSeconds: 30
        resources:
          requests:
            cpu: 50m
            memory: 128Mi
          limits:
            cpu: 100m
            memory: 256Mi
        volumeMounts:
        - name: alertmanager-config
          mountPath: /etc/alertmanager
        - name: alertmanager-data
          mountPath: /alertmanager
      volumes:
      - name: alertmanager-config
        configMap:
          name: alertmanager-config
      - name: alertmanager-data
        persistentVolumeClaim:
          claimName: alertmanager-data
---
apiVersion: v1
kind: Service
metadata:
  name: alertmanager
  labels:
    app: alertmanager
spec:
  ports:
  - port: 9093
    targetPort: http
    protocol: TCP
    name: http
  selector:
    app: alertmanager
  type: ClusterIP
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: alertmanager-data
  labels:
    app: alertmanager
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: standard
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  labels:
    app: alertmanager
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m
      # Customize these values for your preferred notification channels
      # slack_api_url: 'https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX'
      # smtp_smarthost: 'smtp.example.org:587'
      # smtp_from: 'alertmanager@example.org'
      # smtp_auth_username: 'alertmanager'
      # smtp_auth_password: 'password'

    templates:
    - '/etc/alertmanager/template/*.tmpl'

    route:
      group_by: ['alertname', 'job']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'web.hook'
      routes:
      - match:
          severity: critical
        receiver: 'critical-alerts'
        continue: true
      - match:
          severity: warning
        receiver: 'warning-alerts'
        continue: true

    receivers:
    - name: 'web.hook'
      webhook_configs:
      - url: 'http://127.0.0.1:5001/'
        send_resolved: true
    - name: 'critical-alerts'
      # Customize these receivers with your preferred notification channels
      # slack_configs:
      # - channel: '#alerts-critical'
      #   send_resolved: true
      # email_configs:
      # - to: 'alerts-critical@example.org'
      #   send_resolved: true
    - name: 'warning-alerts'
      # slack_configs:
      # - channel: '#alerts-warning'
      #   send_resolved: true
      # email_configs:
      # - to: 'alerts-warning@example.org'
      #   send_resolved: true

    inhibit_rules:
    - source_match:
        severity: 'critical'
      target_match:
        severity: 'warning'
      equal: ['alertname', 'instance']
  
  # Alert rules template file
  prometheus_rules.yml: |
    groups:
    - name: stocker-api-alerts
      rules:
      - alert: HighRequestLatency
        expr: stocker_response_time_ms{stat="avg"} > 500
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High request latency on {{ $labels.path }}"
          description: "{{ $labels.path }} has a response time above 500ms (current value: {{ $value }}ms)"
      
      - alert: APIHighErrorRate
        expr: rate(stocker_http_requests_total{status=~"5.."}[5m]) / rate(stocker_http_requests_total[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is above 5% (current value: {{ $value }})"
      
      - alert: APIInstanceDown
        expr: up{job="stocker-api"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "API instance is down"
          description: "API instance has been down for more than 1 minute."
      
      - alert: HighCPUUsage
        expr: process_cpu_seconds_total{job="stocker-api"} > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage detected"
          description: "CPU usage is above 80% (current value: {{ $value }})"
      
      - alert: HighMemoryUsage
        expr: process_resident_memory_bytes{job="stocker-api"} / process_virtual_memory_bytes{job="stocker-api"} > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage detected"
          description: "Memory usage is above 80% (current value: {{ $value }})"
      
      - alert: DatabaseConnectionFailure
        expr: stocker_database_connection_failures_total > 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Database connection failures detected"
          description: "There have been {{ $value }} database connection failures in the last minute"
      
      - alert: RateLimitExceeded
        expr: rate(stocker_rate_limit_exceeded_total[5m]) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Rate limit frequently exceeded"
          description: "Rate limit is being exceeded more than 10 times per 5 minutes"
